import os
import random
from dotenv import load_dotenv
from src.crawlers.hacker_news import HackerNewsCrawler
from src.crawlers.techcrunch import TechCrunchCrawler
from src.crawlers.theverge import TheVergeCrawler
from src.crawlers.wired import WiredCrawler
from src.crawlers.arstechnica import ArsTechnicaCrawler
from src.crawlers.venturebeat import VentureBeatCrawler
from src.processor.llm_rewriter import ContentProcessor
from src.publisher.blogger_client import BloggerPublisher

load_dotenv()

def main():
    print("=== AI Feed Automation Started (SEO Optimized) ===")
    
    # ëª¨ë“  í¬ë¡¤ëŸ¬ ëª©ë¡
    all_crawlers = [
        HackerNewsCrawler(),
        TechCrunchCrawler(),
        TheVergeCrawler(),
        WiredCrawler(),
        ArsTechnicaCrawler(),
        VentureBeatCrawler(),
    ]
    
    # ëœë¤ìœ¼ë¡œ 3ê°œ ì†ŒìŠ¤ ì„ íƒ (ë‹¤ì–‘ì„± í™•ë³´)
    selected_crawlers = random.sample(all_crawlers, min(3, len(all_crawlers)))
    print(f"Selected sources: {[type(c).__name__ for c in selected_crawlers]}")
    
    processor = ContentProcessor()
    publisher = BloggerPublisher()
    
    # ì„±ê³µì ìœ¼ë¡œ ë°œí–‰ëœ ê¸€ ëª©ë¡ (ë‚´ë¶€ ë§í¬ìš©)
    published_posts = []
    
    for crawler in selected_crawlers:
        try:
            items = crawler.fetch_latest(limit=1)  # ì†ŒìŠ¤ë‹¹ 1ê°œì”©
            for item in items:
                print(f"\nğŸ“ Processing: {item['title']}")
                
                # ì½˜í…ì¸  ì²˜ë¦¬ (SEO ìµœì í™” ì ìš©)
                processed = processor.process_content(item)
                
                # ë©”íƒ€ ì„¤ëª… ì¶œë ¥ (ë””ë²„ê·¸ìš©)
                if processed.get("meta_description"):
                    print(f"ğŸ“‹ Meta: {processed['meta_description'][:50]}...")
                
                # ë°œí–‰
                link = publisher.post_article(processed, is_draft=False)
                print(f"âœ… Result: {link}")
                
                # ë°œí–‰ ì„±ê³µ ì‹œ ë‚´ë¶€ ë§í¬ ëª©ë¡ì— ì¶”ê°€
                if link and not link.startswith("Error") and not link.startswith("Skipped"):
                    processor.add_recent_post(processed["title"], link)
                    published_posts.append({
                        "title": processed["title"],
                        "url": link
                    })
                    
        except Exception as e:
            print(f"âŒ Error with {type(crawler).__name__}: {e}")
    
    print(f"\n=== Finished: {len(published_posts)} posts published ===")
    
    # ë°œí–‰ëœ ê¸€ ëª©ë¡ ì¶œë ¥
    if published_posts:
        print("\nğŸ“š Published posts:")
        for post in published_posts:
            print(f"  - {post['title']}")
            print(f"    {post['url']}")

if __name__ == "__main__":
    main()
